from langchain_community.document_loaders import PyPDFLoader
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain.tools import tool
from pathlib import Path
import aiohttp
import os
from pydantic import SecretStr
from Agent.Tools.Article import Article
from langchain.text_splitter import CharacterTextSplitter

GOOGLE_API_KEY = SecretStr(os.environ["GOOGLE_API_KEY"])
SERPAPI_API_KEY = SecretStr(os.environ["SERPAPI_API_KEY"])

@tool
async def read_pdf_and_save(path: Path) -> str:
    """
    PDF dosyasÄ±nÄ± iÅŸler ve vektÃ¶r veritabanÄ±na kaydeder.
    Bu iÅŸlemden sonra `query_pdf_store` aracÄ± ile PDF'ten bilgi sorgulanabilir.
    """
    try:
        print(f"PDF iÅŸleme baÅŸladÄ±: {path}")
        loader = PyPDFLoader(path)
        document = loader.load()
        print(f"PDF sayfa sayÄ±sÄ±: {len(document)}")
        text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
        docs = text_splitter.split_documents(document)
        embedding = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
        vectorstore = Chroma.from_documents(
            documents=docs,
            embedding=embedding,
            persist_directory="vectorstore/pdf_store"
        )
        print("PDF baÅŸarÄ±yla iÅŸlendi ve kaydedildi.")
        return "PDF baÅŸarÄ±yla iÅŸlendi."
    except Exception as e:
        print(f"PDF iÅŸleme hatasÄ±: {e}")
        return f"PDF iÅŸleme sÄ±rasÄ±nda hata oluÅŸtu: {e}"
@tool
async def serp_api_search(query: str) -> list[Article]:
    """
    Google'da verilen sorgu ile arama yapar ve sonuÃ§larÄ± Article listesi olarak dÃ¶ndÃ¼rÃ¼r.
    """
    params = {
        'api_key': SERPAPI_API_KEY.get_secret_value(),
        'engine': 'google',
        'q': query
    }
    async with aiohttp.ClientSession() as session:
        async with session.get(
            "https://serpapi.com/search",
            params=params
        ) as response:
            results = await response.json()
    return [Article.from_serpapi_result(result) for result in results['organic_results']]

@tool
async def final_answer(answer: str, tools_used: list[str]) -> dict[str, str | list[str]]:
    """Use this tool to provide a final answer to the user."""
    return {"answer": answer, "tools_used": tools_used}

@tool
async def query_pdf_store(query: str) -> str:
    """
    Ã–nceden `read_pdf_and_save` ile iÅŸlenmiÅŸ PDF'lerdeki bilgileri sorgular.
    Sorguyu yanÄ±tlamak iÃ§in bu araÃ§ kullanÄ±lmadan `final_answer` verilmemelidir.
    """
    embedding = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    vectorstore = Chroma(
        persist_directory="vectorstore/pdf_store",
        embedding_function=embedding
    )
    retriever = vectorstore.as_retriever()
    docs = retriever.invoke(query)  # get_relevant_documents() yerine invoke()
    combined_text = "\n".join([doc.page_content for doc in docs])
    return combined_text


@tool
async def think_through_question(input_text: str) -> str:
    """
    KarmaÅŸÄ±k veya Ã§oklu alt sorular iÃ§eren kullanÄ±cÄ± girdilerini adÄ±m adÄ±m analiz eder.
    Her alt soru iÃ§in uygun araÃ§larÄ± Ã¶nerir ve Ã§Ã¶zÃ¼m sÄ±rasÄ±nÄ± planlar.

    Args:
        input_text (str): KullanÄ±cÄ±nÄ±n girdiÄŸi doÄŸal dilde soru veya talimat.

    Returns:
        str: AdÄ±m adÄ±m dÃ¼ÅŸÃ¼nme ve iÅŸlem planÄ±.
    """
    import re

    # CÃ¼mleyi parÃ§alara ayÄ±r
    parts = re.split(r'\b(?:and|also|&|,|\.|\?)+\s*', input_text, flags=re.IGNORECASE)
    parts = [p.strip() for p in parts if p.strip()]

    reasoning_steps = []
    reasoning_steps.append("ğŸ§  DÃ¼ÅŸÃ¼nme PlanÄ± (Chain of Thought):")
    reasoning_steps.append("1. KullanÄ±cÄ±nÄ±n girdisi analiz ediliyor.")

    if len(parts) > 1:
        reasoning_steps.append(f"2. Toplam {len(parts)} alt soru tespit edildi:")
        for i, sub in enumerate(parts, 1):
            suggestion = " â†’ Uygun araÃ§: "
            if "pdf" in sub.lower():
                suggestion += "`read_pdf_and_save` + `query_pdf_store`"
            elif any(w in sub.lower() for w in ["who", "what", "when", "why", "explain", "define"]):
                suggestion += "`serp_api_search`"
            elif "capital" in sub.lower() or "location" in sub.lower():
                suggestion += "`serp_api_search`"
            else:
                suggestion += "`serp_api_search` veya baÄŸlama gÃ¶re belirlenmeli`"
            reasoning_steps.append(f"   - Alt soru {i}: \"{sub}\"{suggestion}")
    else:
        reasoning_steps.append("2. Tek bir soru tespit edildi.")
        reasoning_steps.append(f"   - \"{input_text}\" â†’ Uygun araÃ§: baÄŸlama gÃ¶re")

    reasoning_steps.append("3. AraÃ§lar yukarÄ±daki sÄ±rayla Ã§aÄŸrÄ±lmalÄ±.")
    reasoning_steps.append("4. SonuÃ§lar birleÅŸtirilip `final_answer` ile Ã¶zetlenmeli.")

    return "\n".join(reasoning_steps)